{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset, Features, Value, ClassLabel\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def classification_dataset(\n",
    "    data_dir: str, target_author: str = \"Hugo\", binary_classification: bool = True\n",
    ") -> DatasetDict:\n",
    "    \n",
    "   \n",
    "    dataset_dict = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    authors = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    author_to_label = {author: 1 if binary_classification and author == target_author else 0 for author in authors}\n",
    "    \n",
    "    print(f\"Author-to-Label Mapping: {author_to_label}\")\n",
    "    \n",
    "    for author in authors:\n",
    "        author_dir = os.path.join(data_dir, author)\n",
    "        \n",
    "        # Read all text files for this author\n",
    "        for file_name in os.listdir(author_dir):\n",
    "            file_path = os.path.join(author_dir, file_name)\n",
    "            \n",
    "            # Read paragraph text from file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                paragraphs = file.readlines()\n",
    "            \n",
    "            # Assign labels based on classification mode\n",
    "            if binary_classification:\n",
    "                label = 1 if author == target_author else 0\n",
    "                dataset_dict[\"text\"].extend(paragraphs)\n",
    "                dataset_dict[\"label\"].extend([label] * len(paragraphs))\n",
    "            else:  # Multi-label classification\n",
    "                dataset_dict[\"text\"].extend(paragraphs)\n",
    "                dataset_dict[\"label\"].extend([authors.index(author)] * len(paragraphs))\n",
    "    \n",
    "    # Shuffle the dataset and split into train, validation, and test\n",
    "    df = pd.DataFrame(dataset_dict).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train_df, valid_df, test_df = np.split(\n",
    "        df, \n",
    "        [int(0.8 * len(df)), int(0.9 * len(df))]  # 80% train, 10% validation, 10% test\n",
    "    )\n",
    "    \n",
    "    # Define dataset features\n",
    "    if binary_classification:\n",
    "        features = Features(\n",
    "            {\n",
    "                \"text\": Value(\"string\"),\n",
    "                \"label\": ClassLabel(num_classes=2, names=[\"Other\", target_author]),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        features = Features(\n",
    "            {\n",
    "                \"text\": Value(\"string\"),\n",
    "                \"label\": ClassLabel(num_classes=len(authors), names=authors),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    dataset_splits = {\n",
    "        \"train\": Dataset.from_pandas(train_df, features=features),\n",
    "        \"validation\": Dataset.from_pandas(valid_df, features=features),\n",
    "        \"test\": Dataset.from_pandas(test_df, features=features),\n",
    "    }\n",
    "    \n",
    "    return DatasetDict(dataset_splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
